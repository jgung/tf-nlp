{
  "comments": "90,750 in CoNLL-2005 training corpus, 2800 steps w/ batch size of 32 is about 1 epoch, 70900 steps is 25 epochs",
  "reader": "conll_2005",
  "checkpoint_steps": 2800,
  "patience": 70000,
  "max_steps": 1000000,
  "batch_size": 32,
  "ema_decay": 0.999,
  "optimizer": {
    "name": "Adadelta",
    "params": {
      "epsilon": 1e-6
    },
    "lr": 1.0,
    "clip": 1.0
  },
  "features": {
    "seq_feat": "word",
    "targets": [
      {
        "name": "gold",
        "key": "gold",
        "indices": {
          "O": 0
        },
        "unknown_word": "O",
        "pad_word": "O"
      }
    ],
    "inputs": [
      {
        "name": "elmo",
        "key": "word"
      },
      {
        "name": "word",
        "key": "word",
        "mapping_funcs": [
          "lower",
          "digit_norm"
        ],
        "config": {
          "dim": 100,
          "initializer": {
            "embedding": "vectors/glove.6B.100d.txt",
            "pkl_path": "word-vectors.pkl",
            "include_in_vocab": 400000,
            "restrict_vocab": true
          }
        }
      },
      {
        "name": "marker",
        "key": "marker",
        "config": {
          "dim": 100
        },
        "indices": {
          "0": 0
        },
        "unknown_word": "0",
        "pad_word": "0"
      }
    ]
  },
  "encoders": [
    {
      "name": "tokens",
      "type": "concat",
      "inputs": [
        "word",
        "elmo",
        "marker"
      ],
      "input_dropout": 0.0
    },
    {
      "name": "alternating_lstm",
      "inputs": [
        "tokens"
      ],
      "type": "dblstm",
      "state_size": 300,
      "encoder_output_dropout": 0.1,
      "encoder_layers": 8
    }
  ],
  "heads": [
    {
      "encoder": "alternating_lstm",
      "name": "gold",
      "type": "srl",
      "label_smoothing": 0.1
    }
  ]
}